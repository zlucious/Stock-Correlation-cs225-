## Results

#### Goals

Our goals for this project were to follow certain ideas in the paper, "Analysis of Equity Markets: A Graph Theory Approach" by Joshua Abrams at the University of Arizona, to construct a graph representing the connections between stocks using the spearman correlation coefficient. From here, we planned to do traversals and algorithms that had tangible meaning to decompose the stock market. Breadth-first search would represent finding all possible connections from a current stock (what this stock in indirectly connected to), Strongly Connected Components would find the various sectors of the market and separate them, and Dijkstra's Algorithm would find the degrees of separation between pairs of nodes (how indirect is the correlation between two nodes?). Ideally, after decomposing the market into sectors, we could compare that to what people think of as sectors (i.e. the tech sector, the finance sector, etc.)

#### Data Acquisition

To acquire the data, we iteratively pinged the Alphavantage API for free intraday stock data (sampled over the last few months) for each ticker in the S&P 500 and saved the time series data to json files. Then we recursively examined the time stamps of the data and made sure that all the time stamps lined up (since there were a smattering of missing data points in the various json files). Then, for every pair of stocks, we used Scipy to calculate the spearman rank correlation coefficient (like a dot product to see the degree to which two vectors are in line, without weight to the magnitude of the data changes). Then, we followed the suggestions at the end of Abrams paper (in the future development section) and implemented a time-delay spearman rank correlation coefficient. This works by finding the max spearman between two time series when one is offset by {0,1,....10} to show the impact from one time series to another in a directed fashion (the order of the arguments is important). Then we saved the directed spearmans and undirected spearmans to csv's for processing. The distributions of the coefficients for both directed and undirected are displayed as histograms in our figures directory.

#### Outcomes

We were able to successfully create the three algorithms we suggested, and they passed our sample test suite, which we created by building fake csv's which were more simple to visually examine. We also confirmed that our graph builds correctly and can adequatly load from a csv. We then took a few samplings from our algorithms on the full dataset, and those results are again displayed with the figures. We developed a command line interface to allow the user to select a csv to load from, and allows them to run various algorithms and see the outputs of the algorithms. 

#### Interpretations

After a few different algorithm runs, we came to several conclusions. Firstly, there were a few stocks which were extremely far away from everything else in the S&P 500, such as TSLA (which didn't have a positive correlation with anything with a magnitude above ~0.5). However, in general, the entire stock market is extremely positively correlated (unsurprising, especially given our small sample of time series data). This means that under almost every situation, the whole graph is one connected component. For instance, with a edge specificity of 0.8 (i.e. reject all edges that don't represent a correlation of 0.8 or greater), 400+ nodes are still in a single connected component. In fact, at 0.89, the graph is still mostly connected, only breaking down at about 0.9, at which point quickly nothing becomes connected. A few of these runs are shown as visualizations through Gephi (which is faster and easier to visually examine over a large sample of hyperparameters). This tells us that with the size of our data and the time series' produced by AlphaVantage, it is difficult to accurately break the market down into sectors mathematically through the spearman rank correlation, and therefore it is impossible to compare that to human-labeled sectors.

#### Future Expirementation

To build on this expirement, we would start by expanding the dataset. The most obvious way to build on our dataset is to buy a dataset of historical data or a better API key. We could also do this by examining data far removed from current day (i.e. 1970 - 2000), however the S&P changes and this would have to be accounted for. Additionally, we were not able to verify how well the time-delay spearman rank correlation coefficient works. This could be done (on a bigger dataset) by examining how well components separate on directed versus undirected graph of the S&P 500. 